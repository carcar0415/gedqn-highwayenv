env
	custom_merge.py
# env/custom_merge_env.py
# è‡ªå®šä¹‰åˆæµåŒºç¯å¢ƒï¼Œé›†æˆCLMHAå†²çªé¢„æµ‹ + å¤šæ™ºèƒ½ä½“åŠ¨ä½œæ§åˆ¶æ”¯æŒ + è‡ªå®šä¹‰å¥–åŠ±å‡½æ•° + åŠ é€Ÿåº¦æ¥å£æ”¯æŒ + ä¸»è·¯ç»“æ„é…ç½®ï¼ˆå…¼å®¹æ—  RoadNetworkï¼‰ + éšæœºè½¦è¾†æ‰°åŠ¨å¢å¼º

import gymnasium as gym
import numpy as np
from highway_env.envs.merge_env import MergeEnv
from model.clmha_model import CLMHA
import torch
from utils.risk_utils import extract_state_sequence
from utils.reward_utils import compute_custom_reward
from utils.risk_utils import extract_structured_obs
from highway_env.road.road import Road
from highway_env.road.lane import StraightLane
from collections import defaultdict
import math
import random

LANE_WIDTH = 4

class CustomMergeEnv(MergeEnv):
    def __init__(self):
        super().__init__(render_mode="human")

        self.config.update({
            "controlled_vehicles": 1,
            "action": {
                "type": "MultiAgentAction",
                "action_config": {
                    "type": "DiscreteMetaAction"
                }
            },
            "acceleration_range": [-3.0, 3.0],
            "max_acceleration": 3.0,
            "max_deceleration": 3.0,
            "policy_frequency": 10,
            "road_length": 500,
            "merge_length": 150,
            "merge_start": 400,
            "lanes_count": 4,
            "merge_lanes": 1,
            "idm_speed_factor": 1.1,
            "vehicles_type": "IDM",
            "random_seed": None
        })
        self.configure(self.config)

        self.clmha = CLMHA(input_dim=8, hidden_dim=64)
        self.clmha.load_state_dict(torch.load("model/clmha_model.pth", map_location="cpu"))
        self.clmha.eval()

        self.seq_len = 5
        self.history = []
        self._last_speeds = {}

def make_road(self):
    network = defaultdict(list)
    road = Road(network=network, np_random=self.np_random, record_history=self.config["show_trajectories"])

    lanes = self.config.get("lanes_count", 4)
    merge_lanes = self.config.get("merge_lanes", 1)
    length = self.config.get("road_length", 500)
    merge_length = self.config.get("merge_length", 150)
    merge_start = self.config.get("merge_start", 400)

    for lane_id in range(3):
        road.network["a", "b"].append(
            StraightLane([0, lane_id * LANE_WIDTH], [length, lane_id * LANE_WIDTH],
                         line_types=["continuous", "striped"])
        )

    for m in range(merge_lanes):
        start = [merge_start, -LANE_WIDTH]
        end = [length, 0.5 * LANE_WIDTH]
        road.network["c", "b"].append(
            StraightLane(start, end, line_types=["none", "striped"])
        )

    # âœ… å…¼å®¹ graph æ–¹æ³•
    def get_closest_lane_index(position, heading=0):
        min_dist = float('inf')
        closest_lane = None
        for (start, end), lanes in road.network.items():
            for idx, lane in enumerate(lanes):
                dist = np.linalg.norm(position - lane.position(0, 0))
                if dist < min_dist:
                    min_dist = dist
                    closest_lane = (start, end, idx)
        return closest_lane

    road.graph = {"a": ["b"], "c": ["b"]}
    road.network.get_closest_lane_index = get_closest_lane_index  # âœ… æ·»åŠ æ–¹æ³•

    self.road = road


    def _make_vehicles(self, n_vehicles=10):
        from highway_env.vehicle.behavior import IDMVehicle
        from highway_env.vehicle.controller import ControlledVehicle
        self.road.vehicles = []

        for _ in range(n_vehicles):
            entry = np.random.choice(["a", "c"])
            lane_id = ("a", "b", np.random.randint(4)) if entry == "a" else ("c", "b", 0)
            lane = self.road.network[lane_id[0], lane_id[1]][lane_id[2]]
            position = lane.position(np.random.uniform(0, lane.length), 50)
            vehicle = IDMVehicle(self.road, position, np.random.uniform(20, 30))
            vehicle.speed *= np.random.uniform(0.85, 1.05)
            self.road.vehicles.append(vehicle)

        # for i in range(self.config["controlled_vehicles"]):
        #     entry = np.random.choice(["a", "c"])
        #     lane_id = ("a", "b", np.random.randint(3)) if entry == "a" else ("c", "b", 0)
        #     lane = self.road.network[lane_id[0], lane_id[1]][lane_id[2]]
        #     position = lane.position(np.random.uniform(0, lane.length), 0)
        #     vehicle = ControlledVehicle(self.road, position, np.random.uniform(22, 32))
        #     self.road.vehicles.insert(0, vehicle)
        for i in range(self.config["controlled_vehicles"]):
            entry = np.random.choice(["a", "c"]) 
            lane_id = ("a", "b", np.random.randint(3)) if entry == "a" else ("c", "b", 0)
            lane = self.road.network[lane_id[0], lane_id[1]][lane_id[2]]
            position = lane.position(np.random.uniform(0, lane.length), 0)
            vehicle = ControlledVehicle(self.road, position, np.random.uniform(22, 32))
            self.road.vehicles.insert(0, vehicle)


    def reset(self, seed=None, **kwargs):
        if seed is None:
            seed = random.randint(0, 99999)
        self.np_random, _ = gym.utils.seeding.np_random(seed)

        self.make_road()
        self._make_vehicles()
        obs, _ = super().reset(config={"vehicles_count": 0}, **kwargs)
        self.history = []
        self._last_speeds = {id(v): v.speed for v in self.road.vehicles}
        return obs

    def step(self, actions):
        obs, rewards, terminated, truncated, info = super().step(actions)
        self.history.append(obs)
        done = terminated or truncated

        if len(self.history) >= self.seq_len:
            input_seq = extract_state_sequence(self.history[-self.seq_len:])
            with torch.no_grad():
                risk_scores = self.clmha(input_seq).squeeze(-1).numpy()
        else:
            risk_scores = np.zeros((len(obs),))

        processed_obs = extract_structured_obs(self.road, self.config["controlled_vehicles"], risk_scores)

        for v in self.road.vehicles:
            last_v = self._last_speeds.get(id(v), v.speed)
            current_v = v.speed
            v.acc_value = (current_v - last_v) / self.config.get("policy_frequency", 10)
            self._last_speeds[id(v)] = current_v

        rewards = []
        for i, v in enumerate(self.road.vehicles[:self.config["controlled_vehicles"]]):
            r = compute_custom_reward(info, v, self.config)
            rewards.append(r)

        info["risk"] = float(np.mean(risk_scores))

        return processed_obs, rewards, terminated, truncated, info
model
	clmha_model.py
# model/clmha_model.py
# CLMHA æ¨¡å‹ï¼šCNN + LSTM + å¤šå¤´æ³¨æ„åŠ›ï¼Œç”¨äºé¢„æµ‹è½¦è¾†å†²çªé£é™©ç­‰çº§

import torch
import torch.nn as nn

class MultiHeadAttention(nn.Module):
    def __init__(self, embed_dim, num_heads):
        super(MultiHeadAttention, self).__init__()
        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)

    def forward(self, x):  # x: [B, T, H]
        attn_output, _ = self.attn(x, x, x)
        return attn_output

class CLMHA(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super(CLMHA, self).__init__()
        self.cnn = nn.Sequential(
            nn.Conv1d(in_channels=input_dim, out_channels=32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv1d(32, 64, kernel_size=3, padding=1),
            nn.ReLU()
        )
        self.lstm = nn.LSTM(input_size=64, hidden_size=hidden_dim, batch_first=True)
        self.attn = MultiHeadAttention(hidden_dim, num_heads=4)
        self.fc = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x):  # è¾“å…¥ x: [B, T, F]
        if x.shape[-1] < 8:
            pad_dim = 8 - x.shape[-1]
            x = torch.nn.functional.pad(x, (0, pad_dim))  # åœ¨æœ€åä¸€ç»´å¡«å……0
            # print(f"âš ï¸ è‡ªåŠ¨è¡¥é½çŠ¶æ€ç»´åº¦ï¼šæ–°x.shape = {x.shape}")

        B, T, F = x.size()
        x = x.reshape(B * T, F).unsqueeze(-1)       # âœ… [B*T, F, 1] â†’ é€šé“æ•° F=input_dim
        cnn_out = self.cnn(x).squeeze(-1)           # âœ… [B*T, 64]
        cnn_out = cnn_out.reshape(B, T, -1)         # âœ… [B, T, 64]
        lstm_out, _ = self.lstm(cnn_out)            # âœ… [B, T, H]
        attn_out = self.attn(lstm_out)              # âœ… [B, T, H]
        output = self.fc(attn_out[:, -1, :])        # âœ… [B, 1]
        return output

	dqn_agent.py
# GE-DQN æ™ºèƒ½ä½“ç±»ï¼ˆèåˆåšå¼ˆç­–ç•¥ + DQNç»“æ„ï¼‰

import torch
import torch.nn as nn
import torch.optim as optim
import random
import numpy as np
from collections import deque

class QNet(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(QNet, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 128), nn.ReLU(),
            nn.Linear(128, 128), nn.ReLU(),
            nn.Linear(128, output_dim)
        )

    def forward(self, x):
        return self.net(x)

class DQNAgent:
    def __init__(self, config):
        # æ£€æŸ¥é…ç½®å‚æ•°
        required_keys = ["learning_rate", "gamma", "state_dim", "batch_size", "train_freq", "target_update_interval"]
        for key in required_keys:
            if key not in config:
                raise ValueError(f"é…ç½®ç¼ºå°‘å¿…è¦å‚æ•°: {key}")

        self.lr = config["learning_rate"]
        self.gamma = config["gamma"]
        self.action_space = [0, 1, 2, 3, 4]

        self.state_dim = config["state_dim"]
        self.action_dim = len(self.action_space)

        self.net = QNet(self.state_dim, self.action_dim)
        self.target_net = QNet(self.state_dim, self.action_dim)
        self.optimizer = optim.Adam(self.net.parameters(), lr=self.lr)

        self.memory = deque(maxlen=10000)
        self.batch_size = config["batch_size"]
        self.train_freq = config["train_freq"]
        self.step_counter = 0
        self.target_update_interval = config["target_update_interval"]

    def choose_action(self, state, nash_action=None, epsilon=0.1):
        """é€‰æ‹©åŠ¨ä½œï¼Œæ”¯æŒ Îµ-è´ªå©ªç­–ç•¥å’Œçº³ä»€åŠ¨ä½œèåˆ"""
        self.net.eval()
        state = torch.FloatTensor(state).unsqueeze(0)

        with torch.no_grad():
            if np.random.rand() < epsilon:
                # Îµ-è´ªå©ªæ¢ç´¢ï¼šéšæœºé€‰åŠ¨ä½œ
                dqn_action = np.random.choice(self.action_space)
            else:
                # æ­£å¸¸ Q ç½‘ç»œå†³ç­–
                q_vals = self.net(state)
                dqn_action = q_vals.argmax().item()

        # èåˆçº³ä»€åŠ¨ä½œï¼ˆå¦‚æä¾›ï¼‰
        if nash_action is not None:
            final_action = int(0.5 * dqn_action + 0.5 * nash_action)
        else:
            final_action = dqn_action

        return int(final_action)

    def replay_buffer_add(self, s, a, r, s_, done):
        """å°†ç»éªŒå­˜å…¥å›æ”¾ç¼“å†²åŒº"""
        self.memory.append((s, a, r, s_, done))

    def _pad_state(self, state, target_dim):
        """è‡ªåŠ¨è¡¥é½çŠ¶æ€ç»´åº¦"""
        if state.shape[1] != target_dim:
            pad = target_dim - state.shape[1]
            state = np.pad(state, ((0, 0), (0, pad)), mode='constant')
        return state

    def train_step(self):
        """æ‰§è¡Œä¸€æ¬¡è®­ç»ƒæ­¥éª¤"""
        if len(self.memory) < self.batch_size:
            return

        batch = random.sample(self.memory, self.batch_size)
        s, a, r, s_, d = zip(*batch)

        # ä½¿ç”¨ numpy.array åŠ é€Ÿè½¬æ¢
        s = np.array(s)
        s_ = np.array(s_)

        # è‡ªåŠ¨ä¿®å¤é”™è¯¯çŠ¶æ€ç»´åº¦
        s = self._pad_state(s, self.state_dim)
        s_ = self._pad_state(s_, self.state_dim)

        # è½¬ä¸º Tensor
        s = torch.FloatTensor(s)
        s_ = torch.FloatTensor(s_)
        a = torch.LongTensor(a).unsqueeze(1)
        r = torch.FloatTensor(r).unsqueeze(1)
        d = torch.FloatTensor(d).unsqueeze(1)

        # Q-Learning æ›´æ–°
        q_eval = self.net(s).gather(1, a)
        with torch.no_grad():
            q_next = self.target_net(s_).max(1)[0].unsqueeze(1)
            q_target = r + self.gamma * (1 - d) * q_next

        loss = nn.MSELoss()(q_eval, q_target)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        # æ›´æ–°ç›®æ ‡ç½‘ç»œ
        self.step_counter += 1
        if self.step_counter % self.target_update_interval == 0:
            self.target_net.load_state_dict(self.net.state_dict())

	dqn_agent.py
import torch
import torch.nn as nn
import torch.optim as optim
import random
import numpy as np
from collections import deque

class QNet(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(QNet, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 128), nn.ReLU(),
            nn.Linear(128, 128), nn.ReLU(),
            nn.Linear(128, output_dim)
        )

    def forward(self, x):
        return self.net(x)

class DQNAgent:
    """
    GE-DQN æ™ºèƒ½ä½“ç±»ï¼ˆèåˆåšå¼ˆç­–ç•¥ + DQNç»“æ„ï¼‰ï¼Œæ”¯æŒè½¯æ›´æ–°ã€æ¢¯åº¦è£å‰ªã€Îµ-è´ªå©ªæ¢ç´¢ã€‚
    """
    def __init__(self, config):
        # æ£€æŸ¥é…ç½®å‚æ•°
        required_keys = ["learning_rate", "gamma", "state_dim", 
                         "batch_size", "train_freq", "target_update_interval"]
        for key in required_keys:
            if key not in config:
                raise ValueError(f"é…ç½®ç¼ºå°‘å¿…è¦å‚æ•°: {key}")

        self.lr = config["learning_rate"]
        self.gamma = config["gamma"]
        self.action_space = [0, 1, 2, 3, 4]

        self.state_dim = config["state_dim"]
        self.action_dim = len(self.action_space)

        # Q ç½‘ç»œä¸ç›®æ ‡ç½‘ç»œ
        self.net = QNet(self.state_dim, self.action_dim)
        self.target_net = QNet(self.state_dim, self.action_dim)
        self.optimizer = optim.Adam(self.net.parameters(), lr=self.lr)

        # ç»éªŒå›æ”¾ç¼“å­˜
        self.memory = deque(maxlen=config.get("memory_size", 10000))
        self.batch_size = config["batch_size"]
        self.train_freq = config["train_freq"]
        self.step_counter = 0
        self.target_update_interval = config["target_update_interval"]

        # è½¯æ›´æ–°ç³»æ•° Ï„
        self.tau = config.get("tau", 0.005)

    def choose_action(self, state, nash_action=None, epsilon=0.1):
        """é€‰æ‹©åŠ¨ä½œï¼Œæ”¯æŒ Îµ-è´ªå©ªç­–ç•¥å’Œçº³ä»€åŠ¨ä½œèåˆ"""
        self.net.eval()
        state = torch.FloatTensor(state).unsqueeze(0)

        with torch.no_grad():
            if np.random.rand() < epsilon:
                dqn_action = np.random.choice(self.action_space)
            else:
                q_vals = self.net(state)
                dqn_action = q_vals.argmax().item()

        # èåˆçº³ä»€åŠ¨ä½œï¼ˆå¦‚æä¾›ï¼‰
        if nash_action is not None:
            final_action = int(0.5 * dqn_action + 0.5 * nash_action)
        else:
            final_action = dqn_action

        return int(final_action)

    def replay_buffer_add(self, s, a, r, s_, done):
        """å°†ç»éªŒå­˜å…¥å›æ”¾ç¼“å†²åŒº"""
        self.memory.append((s, a, r, s_, done))

    def _pad_state(self, state, target_dim):
        """è‡ªåŠ¨è¡¥é½çŠ¶æ€ç»´åº¦"""
        if state.shape[1] != target_dim:
            pad = target_dim - state.shape[1]
            state = np.pad(state, ((0, 0), (0, pad)), mode='constant')
        return state

    def train_step(self):
        """æ‰§è¡Œä¸€æ¬¡è®­ç»ƒæ­¥éª¤ï¼Œè¿”å›å½“å‰ TD loss"""
        if len(self.memory) < self.batch_size:
            return None

        batch = random.sample(self.memory, self.batch_size)
        s, a, r, s_, d = zip(*batch)

        # è½¬ä¸º numpy åŠ é€Ÿè½¬æ¢
        s = np.array(s)
        s_ = np.array(s_)

        # è‡ªåŠ¨ä¿®å¤çŠ¶æ€ç»´åº¦
        s = self._pad_state(s, self.state_dim)
        s_ = self._pad_state(s_, self.state_dim)

        # è½¬ä¸º Tensor
        s = torch.FloatTensor(s)
        s_ = torch.FloatTensor(s_)
        a = torch.LongTensor(a).unsqueeze(1)
        r = torch.FloatTensor(r).unsqueeze(1)
        d = torch.FloatTensor(d).unsqueeze(1)

        # Q-Learning æ›´æ–°
        q_eval = self.net(s).gather(1, a)
        with torch.no_grad():
            q_next = self.target_net(s_).max(1)[0].unsqueeze(1)
            q_target = r + self.gamma * (1 - d) * q_next

        loss = nn.MSELoss()(q_eval, q_target)
        self.optimizer.zero_grad()
        loss.backward()
        # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
        torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=1.0)
        self.optimizer.step()

        # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ Î¸_target â† Ï„ Î¸_online + (1-Ï„) Î¸_target
        for p, p_targ in zip(self.net.parameters(), self.target_net.parameters()):
            p_targ.data.mul_(1.0 - self.tau)
            p_targ.data.add_(self.tau * p.data)

        self.step_counter += 1
        return loss.item()
utils
	game_utils
# utils/game_utils.py
# çº³ä»€å‡è¡¡è¿‘ä¼¼æ±‚è§£ï¼ˆBest Responseï¼‰

import numpy as np
import torch

def compute_nash_action(agents, state_list):
    actions = []
    for i, agent in enumerate(agents):
        # print(f"ğŸ‘‰ state_list[{i}].shape = {np.array(state_list[i]).shape}")  # â† æ·»åŠ è¿™è¡Œæ‰“å°
        
        best_q = -np.inf
        best_a = 0
        for a in agent.action_space:
            state_array = np.array(state_list[i]).flatten()
            state_tensor = torch.FloatTensor(state_array).unsqueeze(0)
            # print(f"ğŸ‘‰ é€å…¥DQNçš„ shape: {state_tensor.shape}")

            q_values = agent.net(state_tensor).detach().numpy().squeeze()
            if q_values[a] > best_q:
                best_q = q_values[a]
                best_a = a
        actions.append(best_a)
    return actions

	reward_utils.py
# utils/reward_utils.py
import numpy as np
import math
from typing import Dict, Any

# === å…¨å±€å¸¸é‡ ===
EPSILON = 1e-5  # é¿å…é™¤é›¶
DEFAULT_CONFIG = {
    "v_max": 20,
    "a_comf": 1.0,
    "s_safe": 5,
    "lambda_I": 0.30,
    "lambda_v": 0.30,
    "lambda_a": 0.30,
    "lambda_d": 0.10,
    "T_min": 1.0,
    "T_max": 5.0,
}

def compute_custom_reward(info: Dict[str, Any], vehicle: Any, config: Dict[str, Any]) -> float:
    """
    è®¡ç®—è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°ã€‚

    å‚æ•°:
        info (dict): ç¯å¢ƒä¿¡æ¯å­—å…¸ï¼Œä¾‹å¦‚ç¢°æ’ã€å˜é“ç­‰äº‹ä»¶ã€‚
        vehicle (object): è½¦è¾†å¯¹è±¡ï¼ŒåŒ…å«é€Ÿåº¦ã€åŠ é€Ÿåº¦ç­‰å±æ€§ã€‚
        config (dict): é…ç½®å­—å…¸ï¼ŒåŒ…å«å¥–åŠ±å‡½æ•°çš„å‚æ•°ã€‚

    è¿”å›:
        float: è®¡ç®—å¾—åˆ°çš„å¥–åŠ±å€¼ã€‚
    """
    # === åˆå¹¶é»˜è®¤é…ç½® ===
    config = {**DEFAULT_CONFIG, **config}

    # === åŸºæœ¬çŠ¶æ€ ===
    v = getattr(vehicle, "speed", 0.0)
    v_T = getattr(vehicle, "target_speed", 15.0)
    v_M = config["v_max"]
    a = getattr(vehicle, "acc_value", 1.5)
    a_comf = config["a_comf"]
    s_CG = getattr(vehicle, "gap", 2.0)
    s_safe = config["s_safe"]

    # === è®¡ç®— I_T (åŸºäº TTC + PET) ===
    I_T = _compute_conflict_index(v, v_T, s_CG, config["T_min"], config["T_max"])

    # === æƒé‡å‚æ•° ===
    lambda_I = config["lambda_I"]
    lambda_v = config["lambda_v"]
    lambda_a = config["lambda_a"]
    lambda_d = config["lambda_d"]

    # === åç½®é¡¹ï¼ˆæƒ©ç½š/å¥–åŠ±ï¼‰===
    w_1 = -1000 if info.get("crash", False) else 0               # ç¢°æ’æƒ©ç½š
    w_2 = -10 if info.get("lane_change", False) else 0           # å˜é“æƒ©ç½š
    w_3 = -5 if info.get("out_of_road", False) else 0            # å‡ºç•Œæƒ©ç½š

    # === æ ¸å¿ƒå¥–åŠ±é¡¹ ===
    R_I = 1 / (I_T * abs(a) + EPSILON)        # å†²çª-åŠ é€Ÿåº¦å¹³è¡¡
    R_v = 1 - abs((v - v_T) / v_M)            # è·Ÿè½¦é€Ÿåº¦åŒ¹é…
    R_a = math.exp(-abs(a) / a_comf)          # èˆ’é€‚åŠ é€Ÿåº¦
    R_d = s_CG / (s_safe + EPSILON)           # å®‰å…¨è·ç¦»

    # === å…¶ä»–æƒ©ç½šé¡¹ ===
    penalty_slow_front = -1 if v_T < 8 else 0      # å‰è½¦è¿‡æ…¢æƒ©ç½š
    penalty_gap_small = -2 if s_CG < s_safe else 0 # è·ç¦»è¿‡è¿‘æƒ©ç½š

    # === åŠ æƒå¥–åŠ±å‡½æ•° ===
    reward = (
        lambda_I * R_I +
        lambda_v * R_v +
        lambda_a * R_a +
        lambda_d * R_d +
        w_1 + w_2 + w_3 +
        penalty_slow_front + penalty_gap_small
    )

    return reward

def _compute_conflict_index(v: float, v_T: float, s_CG: float, T_min: float, T_max: float) -> float:
    """
    è®¡ç®—å†²çªé£é™©æŒ‡æ•° I_Tã€‚

    å‚æ•°:
        v (float): å½“å‰é€Ÿåº¦ã€‚
        v_T (float): ç›®æ ‡é€Ÿåº¦ã€‚
        s_CG (float): å½“å‰è½¦è·ã€‚
        T_min (float): æœ€å°æ—¶é—´é˜ˆå€¼ã€‚
        T_max (float): æœ€å¤§æ—¶é—´é˜ˆå€¼ã€‚

    è¿”å›:
        float: å†²çªé£é™©æŒ‡æ•° I_Tã€‚
    """
    TTC = s_CG / (abs(v_T - v) + EPSILON) if abs(v_T - v) > EPSILON else float('inf')
    PET = (s_CG / (v + EPSILON)) - (s_CG / (v_T + EPSILON)) if v_T > EPSILON else float('inf')
    omega_1, omega_2 = 0.5, 0.5  # æƒé‡
    T = omega_1 * TTC + omega_2 * PET

    if T < T_min:
        return 1.0
    elif T > T_max:
        return 0.0
    else:
        return 1 - (T - T_min) / (T_max - T_min)
	
	risk_utils.py
# utils/risk_utils.py
# åŒ…å«å†²çªæŒ‡æ ‡è®¡ç®—ï¼ˆTTC/PETï¼‰ä¸çŠ¶æ€åºåˆ—é¢„å¤„ç†å‡½æ•° + æ”¯æŒå‘¨å›´è½¦è¾†ä¿¡æ¯æå–

import numpy as np
import torch

def extract_state_sequence(history):
    state_seq = np.array(history)  # shape: [T, N, F]
    state_seq = np.transpose(state_seq, (1, 0, 2))  # è½¬ä¸º [N, T, F]
    return torch.FloatTensor(state_seq)  # è¾“å‡ºï¼šæ¯è¾†è½¦çš„çŠ¶æ€åºåˆ—ï¼Œé€‚åˆé€å…¥ CLMHA

def preprocess_obs(obs_list, expected_num=3):
    processed = []
    for i in range(expected_num):
        if i >= len(obs_list) or obs_list[i] is None or obs_list[i] == []:
            processed.append(np.zeros(40))
            continue

        obs_arr = np.array(obs_list[i])

        if obs_arr.ndim == 2:
            flat = obs_arr.flatten()
        elif obs_arr.ndim == 1:
            flat = obs_arr
        else:
            processed.append(np.zeros(40))
            continue

        if flat.shape[0] < 40:
            flat = np.pad(flat, (0, 40 - flat.shape[0]))
        elif flat.shape[0] > 40:
            flat = flat[:40]

        processed.append(flat)
    return processed

def extract_structured_obs(road, expected_num, risk_scores):
    structured = []

    if len(risk_scores) < expected_num:
        risk_scores = list(risk_scores) + [0] * (expected_num - len(risk_scores))

    for i in range(expected_num):
        if i >= len(road.vehicles):
            structured.append(np.zeros(40))
            continue

        ego = road.vehicles[i]
        ego_obs = [
            ego.position[0], ego.position[1], ego.speed,
            ego.heading, ego.lane_index[2] if ego.lane_index else -1,
            risk_scores[i], ego.target_speed, ego.acc_value if hasattr(ego, "acc_value") else 0
        ]

        nearby = sorted(
            [v for j, v in enumerate(road.vehicles) if j != i],
            key=lambda v: np.linalg.norm(v.position - ego.position)
        )[:4]

        for v in nearby:
            rel_pos = v.position - ego.position
            ego_obs += [rel_pos[0], rel_pos[1], v.speed,
                        v.lane_index[2] if v.lane_index else -1,
                        v.acc_value if hasattr(v, "acc_value") else 0,
                        v.heading - ego.heading, np.linalg.norm(rel_pos)]

        if len(ego_obs) < 40:
            ego_obs += [0] * (40 - len(ego_obs))
        elif len(ego_obs) > 40:
            ego_obs = ego_obs[:40]

        structured.append(np.array(ego_obs))

    return structured
	

main_train.py
# main_train.py (Modified with Epsilon Decay, Reward Scaling, Convergence Detection)

# import logging
# logging.basicConfig(level=logging.ERROR)

from env.custom_merge_env import CustomMergeEnv
from model.dqn_agent import DQNAgent
from utils.risk_utils import extract_structured_obs
from utils.game_utils import compute_nash_action
from utils.reward_utils import _compute_conflict_index    # ç”¨äºè®¡ç®—å†²çªæŒ‡æ•° I_T
import yaml, os, numpy as np, torch, sys
import matplotlib.pyplot as plt
import pandas as pd

# â€”â€” ä¿å­˜ä¸Šä¸€é›†çš„å¹³å‡é£é™©ä¸å¹³å‡æˆåŠŸæ¢é“æ¬¡æ•°ï¼ˆç”¨äºä¸‹ä¸€é›†è¾“å…¥ï¼‰
previous_avg_risk = 0.0
previous_avg_lane_changes = 0.0

# â€”â€” æ”¶æ•›æ£€æµ‹å‚æ•°ï¼ˆå¯è°ƒæ•´ï¼‰
# window_size: æ¯å¤šå°‘é›†æ£€æµ‹ä¸€æ¬¡æ”¶æ•›ï¼Œeps_r: reward æ ‡å‡†å·®é˜ˆå€¼ï¼Œeps_l: loss å‡å€¼å˜åŒ–é˜ˆå€¼
window_size = 50
eps_reward = 0.02
eps_loss = 1e-3
reward_history = []
loss_history = []

def check_convergence(rewards, losses, window, eps_r, eps_l):
    if len(rewards) < 2 * window or len(losses) < 2 * window:
        return False
    recent_r = np.array(rewards[-window:])
    if recent_r.std() > eps_r:
        return False
    l1 = np.array(losses[-window:])
    l0 = np.array(losses[-2*window:-window])
    return abs(l1.mean() - l0.mean()) < eps_l

# ç¯å¢ƒç®€å•æµ‹è¯•
env = CustomMergeEnv()
obs, _ = env.reset()
print(len(env.road.vehicles), env.controlled_vehicles[0].position)
_ = env.step((0,))

# å±è”½ highway-env å†…éƒ¨æ‰“å°
class NullWriter:
    def write(self, arg):
        pass

sys.stdout = NullWriter()
sys.stderr = NullWriter()

# ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
os.makedirs("results", exist_ok=True)

# è¯»å–è®­ç»ƒé…ç½®
trick_cfg_path = "config.yaml"
with open(trick_cfg_path, "r") as f:
    config = yaml.safe_load(f)

# ä»¿çœŸæ¯æ­¥æ—¶é•¿ dtï¼ˆé»˜è®¤ 10 Hzï¼‰
dt = 1.0 / config.get("simulation_frequency", 10)

# Îµ-è´ªå©ªæ¢ç´¢å‚æ•°
epsilon_start = 1.0
epsilon_end = 0.05
epsilon_decay_episodes = config["max_train_episodes"]

# é‡å»ºç¯å¢ƒä¸æ™ºèƒ½ä½“
env = CustomMergeEnv()
agents = [DQNAgent(config) for _ in range(1)]

# ç”¨äºå­˜å‚¨æ¯é›†æŒ‡æ ‡
avg_speeds, avg_accels, avg_gaps = [], [], []
avg_risks, avg_lane_changes, episode_rewards = [], [], []

for episode in range(config["max_train_episodes"]):
    obs = env.reset()
    done = False

    # åˆå§‹åŒ–æ¯è½¦çš„ prev_speed
    for v in env.road.vehicles:
        v.prev_speed = v.speed

    # æœ¬é›†ä¸´æ—¶ç»Ÿè®¡
    speed_list, accel_list, gap_list, risk_list = [], [], [], []
    lane_change_count = 0
    episode_reward = 0
    step_losses = []
    step_idx = 0

    # è®¡ç®—å½“å‰ Îµ å€¼
    epsilon = max(
        epsilon_end,
        epsilon_start - (episode / epsilon_decay_episodes) * (epsilon_start - epsilon_end)
    )

    while not done:
        # â€”â€” åœ¨æœ¬é›†ç¬¬ä¸€æ­¥ï¼Œå°†ä¸Šä¸€é›†çš„æŒ‡æ ‡è¾“å…¥ risk_scores
        if step_idx == 0:
            init_input = previous_avg_risk + previous_avg_lane_changes
            risk_scores = [init_input] * len(agents)
        else:
            risk_scores = [o[-1] if len(o) >= 8 else 0 for o in obs]

        # æå–çŠ¶æ€ä¸åŠ¨ä½œ
        state = extract_structured_obs(
            env.road, expected_num=len(agents), risk_scores=risk_scores
        )
        nash_actions = compute_nash_action(agents, state)
        flat_actions = []
        for i, agent in enumerate(agents):
            a = agent.choose_action(state[i], nash_actions[i], epsilon)
            flat_actions.append(int(a))

        # æˆåŠŸæ¢é“è®¡æ•°
        ego = env.road.vehicles[0]
        old_lane = ego.lane_index[2] if ego.lane_index else None

        # ç¯å¢ƒæ­¥è¿›
        next_obs, reward, terminated, truncated, info = env.step(tuple(flat_actions))
        done = terminated or truncated

        ego = env.road.vehicles[0]
        new_lane = ego.lane_index[2] if ego.lane_index else None
        if old_lane is not None and new_lane is not None and new_lane != old_lane:
            lane_change_count += 1

        # è®°å½•é€Ÿåº¦ã€åŠ é€Ÿåº¦ã€gap
        for v in env.road.vehicles:
            accel = (v.speed - v.prev_speed) / dt
            accel_list.append(accel)
            v.prev_speed = v.speed
            speed_list.append(v.speed)
            gap_list.append(getattr(v, "gap", 0))

        # è®°å½•å†²çªæŒ‡æ•° I_T
        ego = env.road.vehicles[0]
        I_T = _compute_conflict_index(
            ego.speed,
            ego.target_speed,
            getattr(ego, "gap", config.get("s_safe", 5)),
            config.get("T_min", 1.0),
            config.get("T_max", 5.0)
        )
        risk_list.append(I_T)

        # ç´¯è®¡å¥–åŠ± + ç¼©æ”¾
        raw_r = sum(reward) if isinstance(reward, (list, tuple, np.ndarray)) else reward
        scaled_r = np.tanh(raw_r / config.get("reward_scale", 1.0))
        episode_reward += scaled_r

        # ç»éªŒå›æ”¾ä¸è®­ç»ƒ
        for i, agent in enumerate(agents):
            s = np.array(state[i]).flatten()
            s_ = (
                np.array(state[i]).flatten()
                if i >= len(next_obs)
                else np.array(next_obs[i]).flatten()
            )
            agent.replay_buffer_add(s, flat_actions[i], scaled_r, s_, done)
            loss = agent.train_step()
            if loss is not None:
                step_losses.append(loss)

        obs = next_obs
        step_idx += 1

    # æœ¬é›†ç»“æŸåè®¡ç®—å¹³å‡æŒ‡æ ‡
    avg_speeds.append(np.mean(speed_list))
    avg_accels.append(np.mean(accel_list))
    avg_gaps.append(np.mean(gap_list))
    avg_risks.append(np.mean(risk_list))
    avg_lane_changes.append(lane_change_count)
    episode_rewards.append(episode_reward)

    # æ›´æ–°ä¸Šä¸€é›†æŒ‡æ ‡
    previous_avg_risk = avg_risks[-1]
    previous_avg_lane_changes = avg_lane_changes[-1]

    # è®°å½•æ”¶æ•›æ£€æµ‹å†å²
    reward_history.append(episode_reward)
    if step_losses:
        loss_history.append(np.mean(step_losses))
    else:
        loss_history.append(np.nan)

    # æ‰“å°è®­ç»ƒè¿›åº¦
    sys.stdout = sys.__stdout__
    print(f"[Episode {episode+1}/{config['max_train_episodes']}] "
          f"Reward: {episode_reward:.2f}  "
          f"AvgRisk: {previous_avg_risk:.3f}  "
          f"AvgLaneChanges: {previous_avg_lane_changes}  "
          f"Îµ: {epsilon:.2f}" )
    sys.stdout = NullWriter()

    # æ”¶æ•›æ£€æµ‹
    if (episode+1) % window_size == 0:
        if check_convergence(reward_history, loss_history, window_size, eps_reward, eps_loss):
            sys.stdout = sys.__stdout__
            print(f"æ”¶æ•›æ£€æµ‹é€šè¿‡ï¼Œè®­ç»ƒåœ¨ Episode {episode+1} æ—¶æå‰ç»“æŸã€‚")
            sys.stdout = NullWriter()
            break

# ä¿å­˜æ¨¡å‹
sys.stdout = sys.__stdout__
for i, agent in enumerate(agents):
    torch.save(agent.net.state_dict(), f"results/dqn_agent_{i}.pth")
print("âœ… æ¨¡å‹å·²ä¿å­˜è‡³ results/")

# å¯¼å‡ºæ‰€æœ‰æŒ‡æ ‡åˆ° CSV
df = pd.DataFrame({
    "episode": list(range(1, config["max_train_episodes"]+1)),
    "avg_speed": avg_speeds,
    "avg_accel": avg_accels,
    "avg_gap": avg_gaps,
    "avg_risk": avg_risks,
    "avg_lane_change": avg_lane_changes,
    "episode_reward": episode_rewards,
})
df.to_csv("results/training_metrics.csv", index=False)
print("âœ… æŒ‡æ ‡å·²ä¿å­˜è‡³ results/training_metrics.csv")

# â€”â€” æ¯ 10 é›†èšåˆå¹¶ç»˜åˆ¶æŠ˜çº¿å›¾ â€”â€” #
window = 100

def aggregate(data, w):
    return [np.mean(data[i:i+w]) for i in range(0, len(data), w)]

x_vals = list(range(window, len(avg_speeds) + 1, window))

plots = [
    (avg_speeds, "æ¯100é›†å¹³å‡é€Ÿåº¦è¶‹åŠ¿å›¾",   "Speed (m/s)",         "results/agg_avg_speed_curve.png"),
    (avg_accels, "æ¯100é›†å¹³å‡åŠ é€Ÿåº¦è¶‹åŠ¿å›¾", "Acceleration (m/sÂ²)",  "results/agg_avg_accel_curve.png"),
    (avg_gaps,   "æ¯100é›†å¹³å‡è½¦è·è¶‹åŠ¿å›¾",   "Gap (m)",              "results/agg_avg_gap_curve.png"),
    (avg_risks,  "æ¯100é›†å¹³å‡é£é™©è¶‹åŠ¿å›¾",   "Risk",                 "results/agg_avg_risk_curve.png"),
    (avg_lane_changes, "æ¯100é›†å¹³å‡æˆåŠŸæ¢é“æ¬¡æ•°è¶‹åŠ¿å›¾","Lane Changes",       "results/agg_avg_lane_change_curve.png"),
    (episode_rewards,  "æ¯100é›†å¹³å‡å¥–åŠ±è¶‹åŠ¿å›¾",   "Total Reward",         "results/agg_avg_reward_curve.png"),
]

for data, title, ylabel, fname in plots:
    y = aggregate(data, window)
    plt.figure()
    plt.plot(x_vals, y, linestyle='-', marker='o', linewidth=1)
    plt.title(title)
    plt.xlabel("Episode")
    plt.ylabel(ylabel)
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(fname)
    plt.close()

print("âœ… æ¯100é›†èšåˆå›¾å·²ä¿å­˜è‡³ results/ï¼ˆä»¥ agg_ å¼€å¤´ï¼‰")

main_test.py
# main_test.py (Modified to plot metrics every 10 episodes, fixed syntax errors)
from env.custom_merge_env import CustomMergeEnv
from model.dqn_agent import DQNAgent
from utils.risk_utils import extract_structured_obs
from utils.reward_utils import _compute_conflict_index
import yaml
import numpy as np
import torch
from PIL import Image
import os
import matplotlib.pyplot as plt

# ==== é…ç½®åŠ è½½ ====
with open("config.yaml", "r") as f:
    config = yaml.safe_load(f)

os.makedirs("results", exist_ok=True)

# ==== æµ‹è¯•å‚æ•° ====
dt = 1.0 / config.get("simulation_frequency", 10)
plot_interval = 10  # æ¯10ä¸ªepisodeè¾“å‡ºä¸€æ¬¡å›¾
max_episodes = config.get("max_test_episodes", 50)

# ==== åˆå§‹åŒ–ç¯å¢ƒ ====
env = CustomMergeEnv()
env.spec = type("spec", (), {"id": "CustomMerge-v0"})()

# ==== åŠ è½½æ¨¡å‹ ====
agents = [DQNAgent(config) for _ in range(1)]
for i, agent in enumerate(agents):
    model_path = f"results/dqn_agent_{i}.pth"
    agent.net.load_state_dict(torch.load(model_path, map_location="cpu"))
    agent.net.eval()
print("âœ… æ¨¡å‹å·²åŠ è½½")

# ==== æµ‹è¯•å¾ªç¯ ====
for episode in range(1, max_episodes + 1):
    # é‡ç½®ç¯å¢ƒï¼Œè·å–è§‚æµ‹
    obs, _ = env.reset()
    done = False

    # åˆå§‹åŒ–è®°å½•
    speed_list, accel_list, risk_list = [], [], []
    ego = env.road.vehicles[0]
    prev_speed = ego.speed
    frames = []
    step = 0

    while not done:
        # æ„é€ çŠ¶æ€
        risk_scores = [o[-1] if len(o) >= 8 else 0 for o in obs]
        state = extract_structured_obs(env.road, expected_num=len(agents), risk_scores=risk_scores)

        # é€‰æ‹©åŠ¨ä½œï¼ˆçº¯ exploitationï¼‰
        actions = [int(agent.choose_action(state[i], epsilon=0.0)) for i, agent in enumerate(agents)]

        # ç¯å¢ƒæ­¥è¿›
        next_obs, reward, terminated, truncated, info = env.step(tuple(actions))
        done = terminated or truncated

        # ç´¯ç§¯è½¦è¾†çŠ¶æ€ä¸æŒ‡æ ‡
        for v in env.road.vehicles:
            speed_list.append(v.speed)
            accel_list.append(getattr(v, 'acc_value', 0))
            I_T = _compute_conflict_index(
                v.speed,
                v.target_speed,
                getattr(v, 'gap', config.get('s_safe', 5)),
                config.get('T_min', 1.0),
                config.get('T_max', 5.0)
            )
            risk_list.append(I_T)

        # æ¸²æŸ“ GIF å¸§
        frame = env.render()
        if isinstance(frame, np.ndarray):
            frames.append(Image.fromarray(frame))

        obs = next_obs
        step += 1

    # è®¡ç®—æœ¬é›†å¹³å‡æŒ‡æ ‡
    avg_speed = np.mean(speed_list) if speed_list else 0
    avg_accel = np.mean(accel_list) if accel_list else 0
    avg_risk = np.mean(risk_list) if risk_list else 0
    print(f"[Episode {episode}/{max_episodes}] å®Œæˆ, steps={step}, "
          f"AvgSpeed={avg_speed:.2f} m/s, "
          f"AvgAccel={avg_accel:.2f} m/sÂ², "
          f"AvgRisk={avg_risk:.3f}")

    # æ¯ plot_interval episodes è¾“å‡ºä¸‰å¼ æŠ˜çº¿å›¾
    if episode % plot_interval == 0:
        t = list(range(len(speed_list)))
        # é€Ÿåº¦æ›²çº¿
        plt.figure()
        plt.plot(t, speed_list, '-o', markersize=3)
        plt.title(f"Episode {episode} é€Ÿåº¦éšæ—¶é—´å˜åŒ–")
        plt.xlabel("Step")
        plt.ylabel("Speed (m/s)")
        plt.grid(True)
        plt.tight_layout()
        plt.savefig(f"results/speed_episode_{episode}.png")
        plt.close()

        # åŠ é€Ÿåº¦æ›²çº¿
        plt.figure()
        plt.plot(t, accel_list, '-o', markersize=3)
        plt.title(f"Episode {episode} åŠ é€Ÿåº¦éšæ—¶é—´å˜åŒ–")
        plt.xlabel("Step")
        plt.ylabel("Accel (m/sÂ²)")
        plt.grid(True)
        plt.tight_layout()
        plt.savefig(f"results/accel_episode_{episode}.png")
        plt.close()

        # é£é™©æ›²çº¿
        plt.figure()
        plt.plot(t, risk_list, '-o', markersize=3)
        plt.title(f"Episode {episode} Risk éšæ—¶é—´å˜åŒ–")
        plt.xlabel("Step")
        plt.ylabel("Risk")
        plt.grid(True)
        plt.tight_layout()
        plt.savefig(f"results/risk_episode_{episode}.png")
        plt.close()

        print(f"âœ… å·²ç”Ÿæˆ Episode {episode} çš„é€Ÿåº¦ã€åŠ é€Ÿåº¦å’Œé£é™©æ›²çº¿å›¾")

    # ä¿å­˜ GIF
    if frames:
        gif_path = f"results/test_episode_{episode}.gif"
        frames[0].save(
            gif_path,
            save_all=True,
            append_images=frames[1:],
            duration=100,
            loop=0
        )
        print(f"âœ… ä¿å­˜ GIF: {gif_path}")

print("æµ‹è¯•å®Œæˆã€‚")
